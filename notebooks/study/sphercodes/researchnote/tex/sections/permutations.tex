\section{Well-distributed permutations}


\subsection{Testing the dependence of the target on subsets of features}

For random variables
$X$, $Y$,
let
$I(X, Y) \geq 0$
be a non-negative measure of probabilitstic dependence,
such that
$I(X, Y) = 0$
if and only if
$X$ and $Y$ are independent.
Moreover,
we assume that,
given three random varibales
$X$, $Y$, $Z$,
we can informally interpret the inequality
$I(X,Y) \leq I(X,Z)$
as stating that
$X$ is "more dependent" on $Z$ than it is on $Y$.

Let
$X_i$, $i=1, \dots, d$
be the $d$ features to select from,
and let $Y$ be the target.


For indices $i_1, i_2, \dots, i_k$, we let
$I(X_{i_1}, \dots, X_{i_k}, y)$
be the measure of probabilistic dependence
between
the vector
$(X_{i_1}, \dots, X_{i_k})$
and
the target $Y$.

\begin{algorithm}[h!]
\caption{Feature selection based on measure $I$ of probabilistic dependence}
\label{algo.featsel}
\begin{algorithmic}[1]
	\REQUIRE
$\sigma^{(1)}, \dots , \sigma^{(p)}$
permutations of length $d$.
\STATE
For
$\ell = 1, \dots, p$
and
$k = 1, \dots, d$
compute
$$
I^{(\ell)}_{k} : =
I(X_{\sigma^{(\ell)}_{1}}, \dots, X_{\sigma^{(\ell)}_{k}}, Y).
$$
\STATE
Maximise
$$
\max
\left\lbrace
I^{(\ell)}_{k} :
\,\,
\ell = 1, \dots, p,
\,
k = 1, \dots, d
\right\rbrace.
$$
\STATE
Let $\hat{\ell}$, $\hat{k}$ be the maximisers.
\RETURN
$$
\sigma^{(\hat{\ell})}_{1}, \dots, \sigma^{(\hat{\ell})}_{\hat{k}}.
$$
\end{algorithmic}
\end{algorithm}

The feature selection procedure in Algorithm \ref{algo.featsel} is fully specified when we give

1) A way to choose the $p$ permutations
$\sigma^{(1)}, \dots , \sigma^{(p)}$
in
$\mathfrak{S}_d$;

2) The measure of probabilistic dependence $I$.


In this note, 
we will not be concerned with 2). 
Examples of measures $I$ that are used in practice are
mutual information for discrete variables,
\weblink{https://arxiv.org/abs/cond-mat/0305641}{KSG estimator} of mutual information for continuous variables,
\weblink{https://en.wikipedia.org/wiki/Adjusted_mutual_information}{adjusted mutual information} for clusterings,
\weblink{http://www.gatsby.ucl.ac.uk/~gretton/papers/GreBouSmoSch05.pdf}{Hilbert-Schmidt Independence Criterion}. 

Regarding 1),
notice the following trade-off.
On the one hand,
we would like to choose
a large number $p$ of permutations
so that 
it is likely that 
there exists a permutation $\hat{\sigma}$ 
such that
its first $k$ entries correspond to 
the "correct" selection of features 
used in the prediction of $Y$.
On the other hand,
we would like to choose
$p$ as small as possible
so that
the computational cost of our 
feature selection procedure is
minimised.
Given this trade-off, 
we would like to choose permutations 
that are "well-distributed" in
$\mathfrak{S}_d$.


The rest of this note
will
examine 
a definition of 
well-distributedness of $p$ permutations in $\mathfrak{S}_d$,
and it will state research questions related to 
the
feature selection procedure
based on such well-distributed permutations.

\begin{prop}
	\label{prop.deterministicdep}
	Let 
	$1\leq j_1, \dots, j_n\leq d$ 
	be $n$ distinct positive integers smaller than or equal to $d$.
	Assume that 
	there exists a deterministic function $f$ such that
	$Y = f(X_{j_1}, \dots, X_{j_n})$.
	We assume that
	\begin{enumerate}
		\item 
		For every 
		$1\leq i_1 <  \dots < i_k\leq d$
		and
		every $\tau \in \mathfrak{S}_k$
		$$
		I(X_{i_1}, \dots, X_{i_k}, Y)
		= 
		I(X_{i_{\tau(1)}}, \dots, X_{i_{\tau(k)}}, Y);
		$$
		\item
		For every 
		$1\leq i_1 <  \dots < i_k\leq d$
		$$
		I(X_{i_1}, \dots, X_{i_k}, Y)
		\leq
		I(X_{j_1}, \dots, X_{j_n}, Y);
		$$
		\item 
		For every 
		$1\leq m_1 < \dots < m_s \leq n$,
		every
		$1\leq i_1 <  \dots < i_k\leq d$ ,
		and every $t=1, \dots, k-1$
		$$
			I(X_{j_{m_1}}, \dots , X_{j_{m_s}}, X_{i_1}, \dots, X_{i_{t+1}}, Y).
		\leq
			I(X_{j_{m_1}}, \dots , X_{j_{m_s}}, X_{i_1}, \dots, X_{i_t}, Y)
		$$
	\end{enumerate}
	Given $p$ permutations
	$\sigma^{(1)}, \dots, \sigma^{(p)}$
	in
	$\mathfrak{S}_d$,
	then 
	Algorithm \ref{algo.featsel}
	returns $\hat{k}$ features 
	$$
	\sigma^{(\hat{\ell})}_{1}, \dots, \sigma^{(\hat{\ell})}_{\hat{k}}.
	$$
	such that
	\begin{equation}
		\lbrace j_1, \dots, j_n\rbrace
		\subset
		\left
		\lbrace
		\sigma^{(\hat{\ell})}_1,
		\dots
		\sigma^{(\hat{\ell})}_{\hat{k}}
		\right
		\rbrace
	\end{equation}
	and
	the number $\hat{k}$ of features returned 
	is 
	\begin{equation}
		\hat{k}
		=
		\min
		\left\lbrace
		k:
		\quad
		\exists \ell,
		\,\,
		\lbrace j_1, \dots, j_n\rbrace
		\subset
		\left
		\lbrace
		\sigma^{(\ell)}_1,
		\dots
		\sigma^{(\ell)}_k
		\right
		\rbrace
		\right\rbrace.
	\end{equation}
\end{prop}


\subsection{Well-distributed permutations}

\begin{defi}[radius]
	\label{def.radius}
	Let $d$ be a positive integer.
	Let $f$ be uniformly distributed among 
	the non empty subsets of $\lbrace 1, \dots, d\rbrace$, 
	i.e.
	for every non-empty subset $c$ of positive integers smaller than or equal to $d$
	it holds
	$$
	P(f = c) = \frac{1}{2^{d} - 1}.
	$$
	Given $p$ permutations
	$
	\sigma^{(1)}, \dots, \sigma^{(p)}
	$
	in 
	$\mathfrak{S}_d$,
	we define their radius $\rho$ as
	\begin{equation}
		\rho(
		\sigma^{(1)}, \dots, \sigma^{(p)}
		)
		=
		E\left[
			\min
			\left\lbrace
			k - \abs{f}:\,\,
			\exists \ell, \,
			f \subset
			\lbrace
			\sigma^{(\ell)}_1, \dots, \sigma^{(\ell)}_k
			\rbrace
			\right\rbrace
			\right].
	\end{equation}
\end{defi}

\begin{remark}
	In the setting of Proposition \ref{prop.deterministicdep},
	the radius $\rho$ of the $p$ permutations
	$
		\sigma^{(1)}, \dots, \sigma^{(p)}
	$
	is the expected number of irrelevant features that Algorithm \ref{algo.featsel}
	returns, when it uses those $p$ permutations. 
\end{remark}

\begin{defi}[well-distributedness]
	\label{def.welldistributedness}
	Let $d$ and $f$ be as in Definition \ref{def.radius}.
	We say that the $p$ permutations
	$
	\hat{\sigma}^{(1)}, \dots, \hat{\sigma}^{(p)}
	$
	in 
	$\mathfrak{S}_d$
	are well distributed if
	they are minimisers of
	\begin{equation}
		\min
		\left\lbrace
		\rho(
		\sigma^{(1)}, \dots, \sigma^{(p)}
		):
		\quad
		\sigma^{(1)}, \dots, \sigma^{(p)} 
		\,
		\in
		\,
		\mathfrak{S}_d
		\right\rbrace.
	\end{equation}
	The minimum in this equation is called minimal radius of $p$ permutations.
\end{defi}


\begin{example}
For 
$d= 4$, 
and
$p = 3$,
direct inspection shows that 
the minimal radius
is
$1/3$
and 
\begin{equation*}
	\rho\left(
	(1, 2, 3, 4),
	(2, 3, 4, 1),
	(3, 4, 1, 2)
	\right)
	=
	\frac{1}{3}.
\end{equation*}
Therefore, 
the three permutations
$
	(1, 2, 3, 4),
	(2, 3, 4, 1),
	(3, 4, 1, 2)
$
are well-distributed. 
Of the ${d! \choose p} = 2024$ choices of 
$\sigma^{(1)}, \sigma^{(2)}, \sigma^{(3)}$,
224 are well distributed. 
Examples of not well-distributed permutations are
\begin{equation*}
	\begin{split}
	\rho\left(
	(2, 1, 0, 3),
	(2, 1, 3, 0),
	(2, 3, 1, 0)
	\right)
		& =
	\frac{22}{30}
	\\
	\rho\left(
	(2, 1, 3, 0),
	(3, 1, 2, 0),
	(3, 2, 1, 0)
	\right)
		& =
	\frac{26}{30}
	\\
	\rho\left(
	(2, 3, 1, 0),
	(3, 1, 2, 0),
	(3, 2, 1, 0)
	\right)
		& =
	\frac{28}{30}.
	\end{split}
\end{equation*}
\end{example}

