\section{Introduction}

Loosely speaking,
standard methods of feature selection
parse the feature vector
$(x_1, \dots, x_d)$
one feature $x_i$ at a time,
and 
choose feature $x_i$ if
a measure of the probabilistic dependence 
$I(x_i, y)$ between $x_i$ and the regression / classification target $y$
is above a certain threshold. 
For example, 
this is the case of
\weblink{https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_regression.html}{sklearn.feature\_selection.f\_regression},
\weblink{https://scikit-learn.org/stable/modules/generated/sklearn.feature\_selection.mutual\_info\_regression.html}{sklearn.feature\_selection.mutual\_info\_regression}.

These methods are not suitable to capture 
complex dependence of the target on 
an ensemble of features. 
We demonstrate this in a notebook attached to this note. 
Using only categorical features 
$(x_1, \dots, x_d)$
the notebook examines feature selection methods to capture the following dependence
$$
y = \begin{cases}
	1 & \text{ if } x_{k_0} = x_{k_1}
	\\
	0 & \text{ otherwise,}
\end{cases}
$$
where $1\leq k_0 < k_1 \leq d$.
Standard methods fail in this example because,
despite the complete dependence of $y$ on $(x_{k_0}, x_{k_1})$,
it is loosely dependent on either of them individually, 
and the estimation misses that.
If we were to test 
$I((x_{i}, x_{k}), y)$,  
$1\leq i < k \leq d$,
instead of 
$I(x_i, y)$,
$1\leq i \leq d$,
we would catch the dependence. 
Unfortunately,
there are ${d \choose 2}$ measurements $I((x_{i}, x_{k}), y)$,
whereas the measurements $I(x_i, y)$ are only $d$. 
Moreover, 
testing
$I((x_{i}, x_{k}), y)$,  
$1\leq i < k \leq d$,
assumes that we knew that the correct number of features to select was 2. 
This number is unknown in practice. 
Thus, one would fall back to testing
$I((x_{i_1}, \dots,  x_{i_{k}}), y)$,  
$1\leq i_1 <\dots < i_k \leq d$,
and there are $2^d$ of them.

However,
one can sample some of the $2^d$ subsets of features,
measure the dependence of the target on the sampled subsets of features,
and choose the subset associated with the highest dependence. 
If the samples are \emph{meaningfully chosen},
one can hope to select features that are 
``suited for purpose'',
despite without guaranteed optimality.
For example, in the attached notebook,
$k_0 = 14$,
$k_1 = 26$,
and
the subset $\lbrace4, 14, 26\rbrace$ was among the samples,
leading to the selection of these three features. 
This selection is 
``suited for purpose''
in the sense that 
it shrinks the initial features vector of length $d = 30$, 
to the three-dimensional feature vector $(x_4, x_{14}, x_{26})$ 
that still contains all the information necessary to predict $y$. 

The following pages aim to make precise the idea of 
``sampling meaningfully''
from all the possible subsets of features,
and they formulate research questions that arise from the discussion. 

I hope you can consider these questions 
and let me know if in your opinion these questions are worth writing about.





